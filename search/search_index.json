{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Software Availability On ThetaGPU, currently we support the major deep learning frameworks through two paths: 1. Singularity containers, based off of Nvidia's Docker containers 2. Bare-metal source builds The bare-metal builds are so far only available for TensorFlow 2.X versions, with plans to support PyTorch soon. TensorFlow 1.X versions are supported only via Nvidia's containers at this time. Containers As of now, the Nvidia containers with TensorFlow 1.x, 2.x, and PyTorch built against cuda11 , cudnn8 are available in Singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively via: $ singularity exec --nv -B /lus:/lus/lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Home"},{"location":"#data-science-software-availability","text":"On ThetaGPU, currently we support the major deep learning frameworks through two paths: 1. Singularity containers, based off of Nvidia's Docker containers 2. Bare-metal source builds The bare-metal builds are so far only available for TensorFlow 2.X versions, with plans to support PyTorch soon. TensorFlow 1.X versions are supported only via Nvidia's containers at this time.","title":"Data Science Software Availability"},{"location":"#containers","text":"As of now, the Nvidia containers with TensorFlow 1.x, 2.x, and PyTorch built against cuda11 , cudnn8 are available in Singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively via: $ singularity exec --nv -B /lus:/lus/lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Containers"},{"location":"GPU%20Monitoring/","text":"GPU Monitoring Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example. GPU Selection In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-monitoring","text":"Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example.","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-selection","text":"In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Selection"},{"location":"Singularity%20Containers/","text":"Nvidia Containers Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Singularity Containers"},{"location":"Singularity%20Containers/#nvidia-containers","text":"Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Nvidia Containers"},{"location":"balsam/","text":"Balsam Experimental Balsam support on ThetaGPU is under active development and testing A Balsam environment configured for ThetaGPU can be obtained by replacing your home directory configuration and sourcing the setup script: rm -r ~/.balsam source /lus/theta-fs0/software/thetagpu/balsam/setup.sh You may then proceed to initialize and activate Balsam databases as usual: balsam init my-db Jobs submitted with --job-mode serial will use a ThetaGPU job template that places jobs on GPUs with round-robin assignment. To leverage this ability, users should submit jobs with the appropriate node_packing_count . For more details, refer to the balsam documentation .","title":"Balsam"},{"location":"balsam/#balsam","text":"Experimental Balsam support on ThetaGPU is under active development and testing A Balsam environment configured for ThetaGPU can be obtained by replacing your home directory configuration and sourcing the setup script: rm -r ~/.balsam source /lus/theta-fs0/software/thetagpu/balsam/setup.sh You may then proceed to initialize and activate Balsam databases as usual: balsam init my-db Jobs submitted with --job-mode serial will use a ThetaGPU job template that places jobs on GPUs with round-robin assignment. To leverage this ability, users should submit jobs with the appropriate node_packing_count . For more details, refer to the balsam documentation .","title":"Balsam"},{"location":"building_python_packages/","text":"To build python packages for Theta GPU, there are two options: build on top of a bare-metal build (currently not available, but coming soon) or build on top of (and within) a singularity container. Additionally, you can build a new container from nvidia's docker images. Building on top of a container At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages. Reaching the outside world for pip packages You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export http_proxy = http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py Building custom packages Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful. HDF5 You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success! Horovod Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Building Python Packages in a Container"},{"location":"building_python_packages/#building-on-top-of-a-container","text":"At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages.","title":"Building on top of a container"},{"location":"building_python_packages/#reaching-the-outside-world-for-pip-packages","text":"You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export http_proxy = http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py","title":"Reaching the outside world for pip packages"},{"location":"building_python_packages/#building-custom-packages","text":"Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.","title":"Building custom packages"},{"location":"building_python_packages/#hdf5","text":"You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!","title":"HDF5"},{"location":"building_python_packages/#horovod","text":"Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Horovod"},{"location":"data_parallel_training/","text":"Distributed training on ThetaGPU using data parallelism Author: Huihuo Zheng huihuo.zheng@anl.gov There are two schemes for distributed learning: Model parallelization : in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f Data parallelization : in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM In this documentation, we would like to show how to do data parallel training on ThetaGPU. I. Software environement setup We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh II. TensorFlow with Horovod 1) Initialize Horovod import horovod.tensorflow as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank gpus = tf . config . experimental . list_physical_devices ( 'GPU' ) for gpu in gpus : tf . config . experimental . set_memory_growth ( gpu , True ) if gpus : tf . config . experimental . set_visible_devices ( gpus [ hvd . local_rank ()], 'GPU' ) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). opt = tf . train . AdagradOptimizer ( 0.01 * hvd . size ()) 4) Wrap the optimizer with Distributed Optimizer opt = hvd . DistributedOptimizer ( opt ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hooks = [hvd.BroadcastGlobalVariablesHook(0)] 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py III. PyTorch with Horovod It is very similar for PyTorch with Horovod 1) Initialize Horovod import horovod.torch as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank torch . cuda . set_device ( hvd . local_rank ()) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). optimizer = optim . SGD ( model . parameters (), lr = args . lr * hvd . size (), momentum = args . momentum ) 4) Wrap the optimizer with Distributed Optimizer optimizer = hvd . DistributedOptimizer ( optimizer , named_parameters = model . named_parameters (), compression = compression ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) 8) Average metric across all the workers Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average. def tensor_average ( val , name ): tensor = torch . tensor ( val ) if ( with_hvd ): avg_tensor = hvd . allreduce ( tensor , name = name ) else : avg_tensor = tensor return avg_tensor . item () We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/pytorch_mnist.py IV. PyTorch with DDP PyTorch has its own native parallelization library called DDP. We will provide omre details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html. V. MPI Profiling for data parallel training We support two ways for profling the performance of data parallel training. 1) mpitrace library MPI trace allows us to get a flat profling of all the MPI function calls involved during the training. To enable this, one can set the environment variable export LD_PRELOAD = /lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so Then run the application as usual. MPI profiling results will be generated after the run finishes mpi_profile.XXXX.[rank_id]. Below is an example output Data for MPI rank 0 of 8: Times and statistics from MPI_Init() to MPI_Finalize(). ----------------------------------------------------------------------- MPI Routine #calls avg. bytes time(sec) ----------------------------------------------------------------------- MPI_Comm_rank 3 0.0 0.000 MPI_Comm_size 3 0.0 0.000 MPI_Bcast 520 197140.6 0.518 MPI_Allreduce 24561 208138.3 162.080 MPI_Gather 126 4.0 0.363 MPI_Gatherv 126 0.0 0.434 MPI_Allgather 2 4.0 0.000 ----------------------------------------------------------------- MPI task 0 of 8 had the maximum communication time. total communication time = 163.396 seconds. total elapsed time = 187.298 seconds. user cpu time = 4127.728 seconds. system time = 728.100 seconds. max resident set size = 8403.938 MBytes. Rank 0 reported the largest memory utilization : 8403.94 MBytes Rank 0 reported the largest elapsed time : 187.30 sec ----------------------------------------------------------------- Message size distributions: MPI_Bcast #calls avg. bytes time(sec) 126 4.0 0.008 1 8.0 0.000 121 25.0 0.006 30 251.5 0.002 32 512.0 0.002 64 1024.0 0.005 44 2048.0 0.003 29 4092.8 0.003 16 8192.0 0.032 MPI_Allreduce #calls avg. bytes time(sec) 19780 8.0 90.822 4576 24.0 18.239 43 4004.0 0.295 5 2780979.2 0.469 50 8160289.2 20.893 9 11803392.0 0.964 48 28060640.0 3.293 50 64731668.5 27.105 MPI_Gather #calls avg. bytes time(sec) 126 4.0 0.363 The useful information here is the message size distribution. 2) Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser. More details: https://horovod.readthedocs.io/en/stable/timeline_include.html","title":"Data Parallel Training"},{"location":"data_parallel_training/#distributed-training-on-thetagpu-using-data-parallelism","text":"Author: Huihuo Zheng huihuo.zheng@anl.gov There are two schemes for distributed learning: Model parallelization : in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f Data parallelization : in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM In this documentation, we would like to show how to do data parallel training on ThetaGPU.","title":"Distributed training on ThetaGPU using data parallelism"},{"location":"data_parallel_training/#i-software-environement-setup","text":"We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh","title":"I. Software environement setup"},{"location":"data_parallel_training/#ii-tensorflow-with-horovod","text":"1) Initialize Horovod import horovod.tensorflow as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank gpus = tf . config . experimental . list_physical_devices ( 'GPU' ) for gpu in gpus : tf . config . experimental . set_memory_growth ( gpu , True ) if gpus : tf . config . experimental . set_visible_devices ( gpus [ hvd . local_rank ()], 'GPU' ) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). opt = tf . train . AdagradOptimizer ( 0.01 * hvd . size ()) 4) Wrap the optimizer with Distributed Optimizer opt = hvd . DistributedOptimizer ( opt ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hooks = [hvd.BroadcastGlobalVariablesHook(0)] 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py","title":"II. TensorFlow with Horovod"},{"location":"data_parallel_training/#iii-pytorch-with-horovod","text":"It is very similar for PyTorch with Horovod 1) Initialize Horovod import horovod.torch as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank torch . cuda . set_device ( hvd . local_rank ()) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). optimizer = optim . SGD ( model . parameters (), lr = args . lr * hvd . size (), momentum = args . momentum ) 4) Wrap the optimizer with Distributed Optimizer optimizer = hvd . DistributedOptimizer ( optimizer , named_parameters = model . named_parameters (), compression = compression ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) 8) Average metric across all the workers Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average. def tensor_average ( val , name ): tensor = torch . tensor ( val ) if ( with_hvd ): avg_tensor = hvd . allreduce ( tensor , name = name ) else : avg_tensor = tensor return avg_tensor . item () We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/pytorch_mnist.py","title":"III. PyTorch with Horovod"},{"location":"data_parallel_training/#iv-pytorch-with-ddp","text":"PyTorch has its own native parallelization library called DDP. We will provide omre details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html.","title":"IV. PyTorch with DDP"},{"location":"data_parallel_training/#v-mpi-profiling-for-data-parallel-training","text":"We support two ways for profling the performance of data parallel training. 1) mpitrace library MPI trace allows us to get a flat profling of all the MPI function calls involved during the training. To enable this, one can set the environment variable export LD_PRELOAD = /lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so Then run the application as usual. MPI profiling results will be generated after the run finishes mpi_profile.XXXX.[rank_id]. Below is an example output Data for MPI rank 0 of 8: Times and statistics from MPI_Init() to MPI_Finalize(). ----------------------------------------------------------------------- MPI Routine #calls avg. bytes time(sec) ----------------------------------------------------------------------- MPI_Comm_rank 3 0.0 0.000 MPI_Comm_size 3 0.0 0.000 MPI_Bcast 520 197140.6 0.518 MPI_Allreduce 24561 208138.3 162.080 MPI_Gather 126 4.0 0.363 MPI_Gatherv 126 0.0 0.434 MPI_Allgather 2 4.0 0.000 ----------------------------------------------------------------- MPI task 0 of 8 had the maximum communication time. total communication time = 163.396 seconds. total elapsed time = 187.298 seconds. user cpu time = 4127.728 seconds. system time = 728.100 seconds. max resident set size = 8403.938 MBytes. Rank 0 reported the largest memory utilization : 8403.94 MBytes Rank 0 reported the largest elapsed time : 187.30 sec ----------------------------------------------------------------- Message size distributions: MPI_Bcast #calls avg. bytes time(sec) 126 4.0 0.008 1 8.0 0.000 121 25.0 0.006 30 251.5 0.002 32 512.0 0.002 64 1024.0 0.005 44 2048.0 0.003 29 4092.8 0.003 16 8192.0 0.032 MPI_Allreduce #calls avg. bytes time(sec) 19780 8.0 90.822 4576 24.0 18.239 43 4004.0 0.295 5 2780979.2 0.469 50 8160289.2 20.893 9 11803392.0 0.964 48 28060640.0 3.293 50 64731668.5 27.105 MPI_Gather #calls avg. bytes time(sec) 126 4.0 0.363 The useful information here is the message size distribution. 2) Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser. More details: https://horovod.readthedocs.io/en/stable/timeline_include.html","title":"V. MPI Profiling for data parallel training"},{"location":"deephyper/","text":"DeepHyper Experimental DeepHyper support on ThetaGPU is under active development and testing. Currently, only single node runs using --evaluator subprocess have been tested. A Python environment with DeepHyper installation can be obtained by sourcing the setup script: source /lus/theta-fs0/software/thetagpu/balsam/setup.sh From a ThetaGPU compute node, you may then initiate a local hyperparameter search using the subprocess evaluator. For instance: deephyper start-project hps_demo cd hps_demo/hps_demo deephyper new-problem hps polynome2 deephyper hps ambs --problem hps_demo.polynome2.problem.Problem --run hps_demo.polynome2.model_run.run --evaluator subprocess For more details refer to the deephyper documentation .","title":"DeepHyper"},{"location":"deephyper/#deephyper","text":"Experimental DeepHyper support on ThetaGPU is under active development and testing. Currently, only single node runs using --evaluator subprocess have been tested. A Python environment with DeepHyper installation can be obtained by sourcing the setup script: source /lus/theta-fs0/software/thetagpu/balsam/setup.sh From a ThetaGPU compute node, you may then initiate a local hyperparameter search using the subprocess evaluator. For instance: deephyper start-project hps_demo cd hps_demo/hps_demo deephyper new-problem hps polynome2 deephyper hps ambs --problem hps_demo.polynome2.problem.Problem --run hps_demo.polynome2.model_run.run --evaluator subprocess For more details refer to the deephyper documentation .","title":"DeepHyper"},{"location":"gpu_node_queue_and_policy/","text":"GPU Node Queue and Policy Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary. Nodes vs Queue vs MIG mode The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script. Queues There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 12 hours MaxRunning will be 2 full nodes or 16 individual GPUs Queue Restrictions MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"GPU Node Queues and Policy"},{"location":"gpu_node_queue_and_policy/#gpu-node-queue-and-policy","text":"Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.","title":"GPU Node Queue and Policy"},{"location":"gpu_node_queue_and_policy/#nodes-vs-queue-vs-mig-mode","text":"The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script.","title":"Nodes vs Queue vs MIG mode"},{"location":"gpu_node_queue_and_policy/#queues","text":"There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 12 hours MaxRunning will be 2 full nodes or 16 individual GPUs","title":"Queues"},{"location":"gpu_node_queue_and_policy/#queue-restrictions","text":"MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"Queue Restrictions"},{"location":"jupyter/","text":"Jupyter Instructions From a thetalogin node: ssh thetagpusn1 to login to a thetaGPU login node. From thetagpusn1 , start an interactive job (make sure to note which thetaGPU node the job gets routed to, thetagpu21 in this example): ( thetagpusn1 ) $ qsub -I -A datascience -n 1 -t 01 :00 -O interactive --attrs = pubnet = true Job routed to queue \"full-node\" . Wait for job 10003623 to start... Opening interactive session to thetagpu21 From the thetaGPU compute node, start a jupyter notebook: Note: This assumes you're in a suitable python environment containing jupyter , for more information on setting up a conda environment, see Running Tensorflow with Conda ): ( thetagpu21 ) $ jupyter notebook & From a new terminal (on your local machine): $ export PORT_NUM = 8889 # any number besides 8888 (the default) should work $ ssh -L $PORT_NUM :localhost:8888 username@theta.alcf.anl.gov ( thetalogin ) $ ssh -L 8888 :localhost:8888 thetagpusn1 ( thetagpusn1 ) $ ssh -L 8888 :localhost:8888 thetagpu21 Navigating to localhost:8889 (or whatever port number you chose above) on your local machine should then establish a connection to the jupyter backend!","title":"Jupyter"},{"location":"jupyter/#jupyter-instructions","text":"From a thetalogin node: ssh thetagpusn1 to login to a thetaGPU login node. From thetagpusn1 , start an interactive job (make sure to note which thetaGPU node the job gets routed to, thetagpu21 in this example): ( thetagpusn1 ) $ qsub -I -A datascience -n 1 -t 01 :00 -O interactive --attrs = pubnet = true Job routed to queue \"full-node\" . Wait for job 10003623 to start... Opening interactive session to thetagpu21 From the thetaGPU compute node, start a jupyter notebook: Note: This assumes you're in a suitable python environment containing jupyter , for more information on setting up a conda environment, see Running Tensorflow with Conda ): ( thetagpu21 ) $ jupyter notebook & From a new terminal (on your local machine): $ export PORT_NUM = 8889 # any number besides 8888 (the default) should work $ ssh -L $PORT_NUM :localhost:8888 username@theta.alcf.anl.gov ( thetalogin ) $ ssh -L 8888 :localhost:8888 thetagpusn1 ( thetagpusn1 ) $ ssh -L 8888 :localhost:8888 thetagpu21 Navigating to localhost:8889 (or whatever port number you chose above) on your local machine should then establish a connection to the jupyter backend!","title":"Jupyter Instructions"},{"location":"mpi/","text":"Launching a Singularity container with MPI #!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"MPI"},{"location":"mpi/#launching-a-singularity-container-with-mpi","text":"#!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"Launching a Singularity container with MPI"},{"location":"python_venvs/","text":"Python virtual environments Base Python3.8 installation A Miniconda base environment using Python 3.8 and containing optimized builds of Tensorflow and Horovod is available by sourcing the setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh Extending with virtualenv To extend this base environment with virtualenv and inherit the base enviroment packages, one can use the --system-site-packages flag: python -m venv --system-site-packages my_env source my_env/bin/activate # Install additional packages here... Extending with conda If you prefer to use conda to manage your environment, refer to the conda environments page for instructions on cloning the base environment. Combining virtualenvs with containers If you wish to extend the Python environment built into a Singularity container, refer to the notes on building on top of a container .","title":"Python Virtual Environments"},{"location":"python_venvs/#python-virtual-environments","text":"","title":"Python virtual environments"},{"location":"python_venvs/#base-python38-installation","text":"A Miniconda base environment using Python 3.8 and containing optimized builds of Tensorflow and Horovod is available by sourcing the setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh","title":"Base Python3.8 installation"},{"location":"python_venvs/#extending-with-virtualenv","text":"To extend this base environment with virtualenv and inherit the base enviroment packages, one can use the --system-site-packages flag: python -m venv --system-site-packages my_env source my_env/bin/activate # Install additional packages here...","title":"Extending with virtualenv"},{"location":"python_venvs/#extending-with-conda","text":"If you prefer to use conda to manage your environment, refer to the conda environments page for instructions on cloning the base environment.","title":"Extending with conda"},{"location":"python_venvs/#combining-virtualenvs-with-containers","text":"If you wish to extend the Python environment built into a Singularity container, refer to the notes on building on top of a container .","title":"Combining virtualenvs with containers"},{"location":"system_overview_and_node_information/","text":"ThetaGPU Machine Overview ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect. Login Nodes The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( ssh thetagpusn1 or ssh thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"System Overview and Node Information"},{"location":"system_overview_and_node_information/#thetagpu","text":"","title":"ThetaGPU"},{"location":"system_overview_and_node_information/#machine-overview","text":"ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect.","title":"Machine Overview"},{"location":"system_overview_and_node_information/#login-nodes","text":"The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( ssh thetagpusn1 or ssh thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"Login Nodes"},{"location":"Building_Compiling/","text":"Description These are the steps to build code that has Python/C++ code interoperability. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: Activate the TensorFlow 2.2 Singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy=https://theta-proxy.tmi.alcf.anl.gov:3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python -m pip install --user virtualenv export VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here python -m virtualenv --system-site-packages $VENV_LOCATION source $VENV_LOCATION/bin/activate python -m pip install cmake python -m pip install matplotlib python -m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Python/C++ code interoperability"},{"location":"Building_Compiling/#description","text":"These are the steps to build code that has Python/C++ code interoperability. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: Activate the TensorFlow 2.2 Singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy=https://theta-proxy.tmi.alcf.anl.gov:3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python -m pip install --user virtualenv export VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here python -m virtualenv --system-site-packages $VENV_LOCATION source $VENV_LOCATION/bin/activate python -m pip install cmake python -m pip install matplotlib python -m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"Building_Compiling/compiling_and_linking/","text":"Compiling and Linking ThetaGPU basically has AMD processors on the service nodes ( thetagpusn1,2 ) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I ), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"Building_Compiling/compiling_and_linking/#compiling-and-linking","text":"ThetaGPU basically has AMD processors on the service nodes ( thetagpusn1,2 ) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I ), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"daskmpi/","text":"Dask-mpi on ThetaGPU How to run Dask-MPI on ThetaGPU at Argonne Leadership Computing Facility. Install Submit a batch job Run a script in an interactive session Start a JupyterLab interactive session Install ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Create a new conda environment conda create -n envname Activate the environment conda activate envname Install required packages conda install -c rapidsai -c nvidia -c numba -c conda-forge \\ cudf cupy python = 3 .7 ipython = 5 .8 cudatoolkit mpi4py dask dask-mpi \\ bokeh jupyter Jupyterlab ipykernel Download the files start_daskmpi.py , cupy_test.py and jupyterlab_cupy_test.ipynb from this repository into a directory, e.g. dirname . Optional: create the Jupyter kernel for interactive JupyterLab sessions env = $( basename ` echo $CONDA_PREFIX ` ) python -m ipykernel install --user --name \" $env \" --display-name \"Python [conda env:\" $env \"]\" Local storage If your dataset is larger than the combined memory of all compute nodes, Dask will spill excess data to disk. If you do not have write permission to local storage on the compute nodes, spilling to disk will be disabled by default, as explained here . Submit a batch job This will run cupy_test.py using 8 GPUs on one node. ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit a batch job on a single node qsub -n 1 -q full-node -A datascience -t 00 :30:00 daskmpi_job_gpu.sh where the script daskmpi_job_gpu.sh is #!/bin/bash source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh conda activate envname cd dirname export PYTHONPATH = '/full/path/to/dirname' mpirun -np 9 python start_daskmpi.py cupy_test.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. on multiple nodes qsub -n 2 -q full-node -A datascience -t 00 :30:00 daskmpi_job_gpu.sh where the script daskmpi_job_gpu.sh is #!/bin/bash source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh conda activate envname cd dirname export PYTHONPATH = '/full/path/to/dirname' mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py cupy_test.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested. Run a script in an interactive session ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit an interactive job on n nodes qsub -n 2 -q full-node -A datascience -I -t 00 :30:00 A shell opens up on one of the compute nodes Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Activate the environment created in the install section conda activate envname cd into the directory dirname where files start_daskmpi.py and cupy_test.py are cd dirname export the directory where cupy_test.py is to $PYTHONPATH export PYTHONPATH = '/full/path/to/dirname' Run the example script cupy_test.py (taken from \" GPU Dask Arrays, first steps: throwing Dask and CuPy together \") single node mpirun -np 9 python start_daskmpi.py cupy_test.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.199:8786 on node thetagpu11 Starting the workers Client status: <Client: 'tcp://172.23.2.199:8786' processes=8 threads=8, memory=270.50 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetagpusn1 ssh -t -L 8787:localhost:8787 thetagpu11 To open the Dask dashboard, go to: http://localhost:8787/status Operation on dask.array (from https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps): with gpus elapsed time: 3.932972 without gpus elapsed time: 47.268787 Code ran successfully. Successfully exited multiple nodes mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py cupy_test.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.192:8786 on node thetagpu04 Starting the workers Client status: <Client: 'tcp://172.23.2.192:8786' processes=16 threads=16, memory=541.00 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetagpusn1 ssh -t -L 8787:localhost:8787 thetagpu04 To open the Dask dashboard, go to: http://localhost:8787/status Operation on dask.array (from https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps): with gpus elapsed time: 4.121156 without gpus elapsed time: 23.829560 Code ran successfully. Successfully exited NB: In the cupy_test.py example the matrix size is not big enough to see a reduction of execution time between 8 GPUs (1 node) and 16 GPUs (2 nodes). Increase the value of variable size in cupy_test.py to make full use of the 16 GPUs. Dask dashboard You can connect to the Dask dashboard on http://localhost:8787/status in you browser after you run the ssh command printed in the above output message in a shell on your local machine. Start a JupyterLab interactive session ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit an interactive job on n nodes qsub -n 2 -q full-node -A datascience -I -t 00 :30:00 A shell opens up on one of the compute nodes Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Activate the environment created in the install section conda activate envname cd into the directory dirname where file start_daskmpi.py is cd dirname Run the start_daskmpi.py script without any argument to start a JupyterLab session single node mpirun -np 9 python start_daskmpi.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. multiple nodes mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.203:8786 on node thetagpu15 Starting the workers Client status: <Client: 'tcp://172.23.2.203:8786' processes=8 threads=8, memory=270.50 GB> Starting JupyterLab on the scheduler... To connect to JupyterLab and Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetagpusn1 ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetagpu15 To open JupyterLab, go to (see log file /home/username/dask_logs/jupyterlab.log): http://localhost:7787/?token=444f71ede62dfe1718f2499baee3483893fc14d6c7e97b88 To open the Dask dashboard, go to: http://localhost:8787/status JupyterLab started. Type 'stop' to stop Dask: JupyterLab and Dask dashboard: You can connect to JupyterLab on http://localhost:7787/ in you browser and view the Dask dashboard on http://localhost:8787/status after you run the ssh command printed in the above output message in a shell on your local machine. You can try the example notebook jupyterlab_cupy_test.ipynb . Type stop to terminate the Dask session.","title":"Dask-mpi on ThetaGPU"},{"location":"daskmpi/#dask-mpi-on-thetagpu","text":"How to run Dask-MPI on ThetaGPU at Argonne Leadership Computing Facility. Install Submit a batch job Run a script in an interactive session Start a JupyterLab interactive session","title":"Dask-mpi on ThetaGPU"},{"location":"daskmpi/#install","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Create a new conda environment conda create -n envname Activate the environment conda activate envname Install required packages conda install -c rapidsai -c nvidia -c numba -c conda-forge \\ cudf cupy python = 3 .7 ipython = 5 .8 cudatoolkit mpi4py dask dask-mpi \\ bokeh jupyter Jupyterlab ipykernel Download the files start_daskmpi.py , cupy_test.py and jupyterlab_cupy_test.ipynb from this repository into a directory, e.g. dirname . Optional: create the Jupyter kernel for interactive JupyterLab sessions env = $( basename ` echo $CONDA_PREFIX ` ) python -m ipykernel install --user --name \" $env \" --display-name \"Python [conda env:\" $env \"]\"","title":"Install"},{"location":"daskmpi/#local-storage","text":"If your dataset is larger than the combined memory of all compute nodes, Dask will spill excess data to disk. If you do not have write permission to local storage on the compute nodes, spilling to disk will be disabled by default, as explained here .","title":"Local storage"},{"location":"daskmpi/#submit-a-batch-job","text":"This will run cupy_test.py using 8 GPUs on one node. ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit a batch job on a single node qsub -n 1 -q full-node -A datascience -t 00 :30:00 daskmpi_job_gpu.sh where the script daskmpi_job_gpu.sh is #!/bin/bash source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh conda activate envname cd dirname export PYTHONPATH = '/full/path/to/dirname' mpirun -np 9 python start_daskmpi.py cupy_test.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. on multiple nodes qsub -n 2 -q full-node -A datascience -t 00 :30:00 daskmpi_job_gpu.sh where the script daskmpi_job_gpu.sh is #!/bin/bash source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh conda activate envname cd dirname export PYTHONPATH = '/full/path/to/dirname' mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py cupy_test.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested.","title":"Submit a batch job"},{"location":"daskmpi/#run-a-script-in-an-interactive-session","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit an interactive job on n nodes qsub -n 2 -q full-node -A datascience -I -t 00 :30:00 A shell opens up on one of the compute nodes Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Activate the environment created in the install section conda activate envname cd into the directory dirname where files start_daskmpi.py and cupy_test.py are cd dirname export the directory where cupy_test.py is to $PYTHONPATH export PYTHONPATH = '/full/path/to/dirname' Run the example script cupy_test.py (taken from \" GPU Dask Arrays, first steps: throwing Dask and CuPy together \") single node mpirun -np 9 python start_daskmpi.py cupy_test.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.199:8786 on node thetagpu11 Starting the workers Client status: <Client: 'tcp://172.23.2.199:8786' processes=8 threads=8, memory=270.50 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetagpusn1 ssh -t -L 8787:localhost:8787 thetagpu11 To open the Dask dashboard, go to: http://localhost:8787/status Operation on dask.array (from https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps): with gpus elapsed time: 3.932972 without gpus elapsed time: 47.268787 Code ran successfully. Successfully exited multiple nodes mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py cupy_test.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.192:8786 on node thetagpu04 Starting the workers Client status: <Client: 'tcp://172.23.2.192:8786' processes=16 threads=16, memory=541.00 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetagpusn1 ssh -t -L 8787:localhost:8787 thetagpu04 To open the Dask dashboard, go to: http://localhost:8787/status Operation on dask.array (from https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps): with gpus elapsed time: 4.121156 without gpus elapsed time: 23.829560 Code ran successfully. Successfully exited NB: In the cupy_test.py example the matrix size is not big enough to see a reduction of execution time between 8 GPUs (1 node) and 16 GPUs (2 nodes). Increase the value of variable size in cupy_test.py to make full use of the 16 GPUs.","title":"Run a script in an interactive session"},{"location":"daskmpi/#dask-dashboard","text":"You can connect to the Dask dashboard on http://localhost:8787/status in you browser after you run the ssh command printed in the above output message in a shell on your local machine.","title":"Dask dashboard"},{"location":"daskmpi/#start-a-jupyterlab-interactive-session","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov ssh into one of ThetaGPU's login nodes ssh thetagpusn1 Submit an interactive job on n nodes qsub -n 2 -q full-node -A datascience -I -t 00 :30:00 A shell opens up on one of the compute nodes Load a pre-defined conda environment (either one works fine) with pytorch source /lus/theta-fs0/software/thetagpu/conda/pt_master/2020-11-25/mconda3/setup.sh with tensorflow source /lus/theta-fs0/software/thetagpu/conda/tf_master/2020-11-11/mconda3/setup.sh Activate the environment created in the install section conda activate envname cd into the directory dirname where file start_daskmpi.py is cd dirname Run the start_daskmpi.py script without any argument to start a JupyterLab session single node mpirun -np 9 python start_daskmpi.py The number of processors ( -np ) should be (at least) the number of GPUs per node plus one. multiple nodes mpirun -x LD_LIBRARY_PATH -x PYTHONPATH -x PATH -np 20 -npernode 10 --hostfile $COBALT_NODEFILE python start_daskmpi.py The number of processors per node ( -npernode ) should be (at least) the number of GPUs per node plus one. The number of processors ( -np ) should be -npernode times the number of nodes requested. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://172.23.2.203:8786 on node thetagpu15 Starting the workers Client status: <Client: 'tcp://172.23.2.203:8786' processes=8 threads=8, memory=270.50 GB> Starting JupyterLab on the scheduler... To connect to JupyterLab and Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetagpusn1 ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetagpu15 To open JupyterLab, go to (see log file /home/username/dask_logs/jupyterlab.log): http://localhost:7787/?token=444f71ede62dfe1718f2499baee3483893fc14d6c7e97b88 To open the Dask dashboard, go to: http://localhost:8787/status JupyterLab started. Type 'stop' to stop Dask: JupyterLab and Dask dashboard: You can connect to JupyterLab on http://localhost:7787/ in you browser and view the Dask dashboard on http://localhost:8787/status after you run the ssh command printed in the above output message in a shell on your local machine. You can try the example notebook jupyterlab_cupy_test.ipynb . Type stop to terminate the Dask session.","title":"Start a JupyterLab interactive session"},{"location":"ml_frameworks/pytorch/running_with_conda/","text":"Running PyTorch with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. One can test these software in an interactive session: qsub -I -n 1 -t 30 -A <project-name> PyTorch (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of PyTorch which includes better performance and support for these architectures. Users can find the latest builds via the module avail pytorch command, which will list available builds such as conda/pytorch/2021-03-02 which is a version of the master branch git repo as of the date 2021-03-02 . This version can be used by module load conda/pytorch/2021-03-02 # loads conda into your environment, sets up appropriate CUDA libraries conda activate # activates conda with python This will setup a conda environment with a recent \"from scratch\" build of the PyTorch repository on the master branch. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . Setup the conda environment you want to use as instructed above. Create/edit your $HOME/.condarc file to include this these lines, replacing <project-name with your project name. By default, Conda will your $HOME/.conda/* area for caching files. Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead. pkgs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /pkgs envs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /envs Clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Activate that environment: conda activate <path/to/env> One should then be able to install modules natively.","title":"Running with Conda"},{"location":"ml_frameworks/pytorch/running_with_conda/#running-pytorch-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. One can test these software in an interactive session: qsub -I -n 1 -t 30 -A <project-name>","title":"Running PyTorch with Conda"},{"location":"ml_frameworks/pytorch/running_with_conda/#pytorch-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of PyTorch which includes better performance and support for these architectures. Users can find the latest builds via the module avail pytorch command, which will list available builds such as conda/pytorch/2021-03-02 which is a version of the master branch git repo as of the date 2021-03-02 . This version can be used by module load conda/pytorch/2021-03-02 # loads conda into your environment, sets up appropriate CUDA libraries conda activate # activates conda with python This will setup a conda environment with a recent \"from scratch\" build of the PyTorch repository on the master branch. This package will also include the latest Horovod tagged release.","title":"PyTorch (master build)"},{"location":"ml_frameworks/pytorch/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/pytorch/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/pytorch/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . Setup the conda environment you want to use as instructed above. Create/edit your $HOME/.condarc file to include this these lines, replacing <project-name with your project name. By default, Conda will your $HOME/.conda/* area for caching files. Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead. pkgs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /pkgs envs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /envs Clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Activate that environment: conda activate <path/to/env> One should then be able to install modules natively.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/","text":"NVidia Container Notes Getting the container To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/ Running on ThetaGPU After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different. Running Tensorflow-2 with Horovod on ThetaGPU To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running with Singularity"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#nvidia-container-notes","text":"","title":"NVidia Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#getting-the-container","text":"To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/","title":"Getting the container"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-on-thetagpu","text":"After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different.","title":"Running on ThetaGPU"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-tensorflow-2-with-horovod-on-thetagpu","text":"To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running Tensorflow-2 with Horovod on ThetaGPU"},{"location":"ml_frameworks/tensorflow/running_with_conda/","text":"Running Tensorflow with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. One can test these software in an interactive session: qsub -I -n 1 -t 30 -A <project-name> Tensorflow (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can find the latest builds via the module avail tensorflow command, which will list available builds such as conda/tensorflow/2021-03-02 which is a version of the master branch git repo as of the date 2021-03-02 . This version can be used by module load conda/tensorflow/2021-03-02 # loads conda into your environment, sets up appropriate CUDA libraries conda activate # activates conda with python This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . Setup the conda environment you want to use as instructed above. Create/edit your $HOME/.condarc file to include this these lines, replacing <project-name> with your project name. By default, Conda will your $HOME/.conda/* area for caching files. Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead. pkgs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /pkgs envs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /envs Clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Activate that environment: conda activate <path/to/env> One should then be able to install modules natively.","title":"Running with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#running-tensorflow-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. One can test these software in an interactive session: qsub -I -n 1 -t 30 -A <project-name>","title":"Running Tensorflow with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#tensorflow-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can find the latest builds via the module avail tensorflow command, which will list available builds such as conda/tensorflow/2021-03-02 which is a version of the master branch git repo as of the date 2021-03-02 . This version can be used by module load conda/tensorflow/2021-03-02 # loads conda into your environment, sets up appropriate CUDA libraries conda activate # activates conda with python This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. This package will also include the latest Horovod tagged release.","title":"Tensorflow (master build)"},{"location":"ml_frameworks/tensorflow/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . Setup the conda environment you want to use as instructed above. Create/edit your $HOME/.condarc file to include this these lines, replacing <project-name> with your project name. By default, Conda will your $HOME/.conda/* area for caching files. Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead. pkgs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /pkgs envs_dirs: - /lus/theta-fs0/projects/<project-name>/conda/ ${ ENV_NAME } /envs Clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Activate that environment: conda activate <path/to/env> One should then be able to install modules natively.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/","text":"Tensorboard Instructions After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/#tensorboard-instructions","text":"After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard Instructions"}]}